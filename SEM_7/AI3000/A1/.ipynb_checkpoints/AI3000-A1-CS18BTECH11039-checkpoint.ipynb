{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI3000 - A1\n",
    "\n",
    "## Raj Patil - CS18BTECH11039\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceeding as follows:\n",
    "\n",
    "### States\n",
    "- as the history matters for a succesful exit but a markov chain has no memory, we need to incorporate the history into the state itself\n",
    "    - setting a state as a record of the last four tosses\n",
    "    - however this is not the smartest way to go about things and has severe computational demands(341 states)\n",
    "    - instead, fixing the history specific to the our target i.e. in terms of equivalence classes of the suffix of a states being equal to prefixes of the desired final state\n",
    "- Hence the number of states will be 5 (prefixes being {\"\",\"1\",\"12\",...\"1234\"})\n",
    "- corresponding to these prefixes, semantically identifying the states as :\n",
    " 1. \"ssss\"\n",
    " 2. \"sss1\"\n",
    " 3. \"ss12\"\n",
    " 4. \"s123\"\n",
    " 5. \"1234\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we need to find the number of expected steps to reach the final state, setting the discount factor as 1 $(\\gamma = 1)$ (as the value of a step is the same in the future compared to the present),  \n",
    "and setting the reward for any transition as $-1$  \n",
    "Formalizing:\n",
    "\n",
    "$$\\mathcal R : \\mathcal S \\rightarrow \\mathbb R$$\n",
    "\n",
    "$$\\forall s \\in (\\mathcal S \\setminus \\{\"1234\"\\}),\\mathcal R(s) = -1 $$\n",
    "$$\\mathcal R(\"1234\") = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to compute the expected number of tosses to reach the state $\"1234\"$, we can calculate the value of the root state $\\epsilon$ using the Bellman equation\n",
    "\n",
    "$$V(s) = \\mathbb E( G_t|s_t=s) = \\mathbb E(r_{t+1} + \\gamma \\cdot V(s_{t+1}|s_t = s)$$\n",
    "\n",
    "as $\\gamma = 1$, can't use the matrix form, hence using dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
